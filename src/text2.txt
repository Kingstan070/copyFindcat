A language model is a statistical model that captures relevant linguistic features of the corpus on which it is trained. At a basic level, it should capture the frequency distribution of letters and words. A more advanced language model should capture syntactic and grammatical dependencies, such as agreement and inflection, and semantic properties, such as which words are likely to occur in a given context. Language models are typically used for two main tasks: scoring and generation. To apply language modeling to plagiarism detection, you can train a language model on a bunch of text that you think people may copy from. Let’s say you are currently teaching about the Gold Foil Experiment in a chemistry class. You could take several relevant Wikipedia articles, the top 10 Google search results for “gold foil experiment”, and perhaps student submission from the last several years (if you’re worried about hand-me-down papers) and dump them in a single text file. This aggregated dataset will be our training data that we use to build a language model, which captures the statistical features of the text.